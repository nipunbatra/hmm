<!doctype html>
<meta charset="utf-8">
<head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://distill.pub/template.v1.js"></script>
               <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/front-matter">
  title: "Tensor Decomposition and its applications"
  description: "Description of the post"
  authors:
  - Rithwik Kukunuri
  - Shivji Bhagat
  affiliations:
  - IIT Gandhinagar: https://iitgn.ac.in/
  - IIT Gandhinagar: https://iitgn.ac.in/
</script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
<script src="references.js" ></script>
<script type="text/bibliography">
  @article{tensorrank,
    title={Tensor rank is NP-complete},
    author={Johan Håstad},
    
    year={1990},
    url={http://www.sciencedirect.com/science/article/pii/0196677490900146},
  },
  @article{tensorrecommendation,
    title={Tensor Decomposition for Signal Processing and Machine Learning
},
    author={Nicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E. Papalexakis, Christos Faloutsos},
    
    year={2016},
    url={https://arxiv.org/abs/1607.01668},
  },
    @article{compression,
    title={  SPEEDING-UP CONVOLUTIONAL NEURAL NETWORKS
USING FINE-TUNED CP-DECOMPOSITION
},
    author={Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky},
    
    year={2015},
    url={https://arxiv.org/abs/1412.6553},
  },

      @article{interaction,
    title={  Understanding the CANDECOMP/PARAFAC Tensor Decomposition, aka CP; with R code
},
    author={Alexej Gossmann},
    
    year={2017},
    url={https://www.alexejgossmann.com/tensor_decomposition_CP/},
  },

@article{tamara,
    title={  Tensor decompositions and applications
},
    author={Tamara G Kolda, Brett W Bader},
    
    year={2009},
    url={https://epubs.siam.org/doi/pdf/10.1137/07070111X},
  },








</script>


<style type="text/css">
  


  body {

    font-family: Roboto!important;

  }

  p {

    font-family: Roboto!important;

  };
  h1 {

    font-family: Roboto!important;

  };
  h2 {

    font-family: Roboto!important;

  };

   h3  {

    font-family: Roboto!important;

  };

h4  {

    font-family: Roboto!important;

  };

h5  {

    font-family: Roboto!important;

  };

h6 {

    font-family: Roboto!important;

  };







  svg{
    z-index: -1;
  }


  .heavy{
    font-weight: 400;
  }

  .slider {
    padding: .5em;
    margin-top: 1em;
  -webkit-appearance: none;
  width: 100%;
  height: 15px;
  border-radius: 5px;
  background: #d3d3d3;
  outline: none;
  opacity: 0.7;
  -webkit-transition: .2s;
  transition: opacity .2s;
  cursor: pointer;
  z-index: 10000;
}

.slider:hover {
  opacity: 1;
  z-index: 10000;
}

.slider::-webkit-slider-thumb {
  z-index: 10000;
  -webkit-appearance: none;
  appearance: none;
  width: 50px;
  height: 50px;
  border-radius: 50%;
  background: #286090;
  cursor: pointer;
}

.slider::-moz-range-thumb {
  width: 50px;
  height: 50px;
  border-radius: 50%;
  background: #286090;
  cursor: pointer;
  z-index: 10000;
}



</style>
</head>

<dt-article class="centered" style="text-align: justify;"> 



  <h1>Tensor Decomposition and its applications</h1>

  
  <h2>What are Tensors?</h2>

  <p>
  Vectors are an extension over scalars and matrices are an extension over vectors. Matrices can also be considered as an an ordered collection of vectors.  Similarly, tensors can be considered as an extension over matrices. A 3-dimensional tensor is an ordered collection of matrices. 


  Tensors can be seen in our day-to-day lives. An image is an example of 3-dimensional tensor, where each color channel is a matrix. Videos are examples of 4-dimensional tensors, where one index corresponds to the time and the other three indices correspond to color channels red, green and blue.
  </p>

  <p><img src='static/lena.svg' class="centered"></p>

  <h2>Notation</h2>
  <p>We use the following notation throughout the article. Scalars are denoted using non-boldfaced lowercase charecters such as 'a'. Vectors (order 1 tensors)  are denoted using lowercase letters suh as \(\mathbf{a}\). Matrices (order 2 tensors) are denoted using capital case letters such as \(A\). Tensors(order 3 and greater) are represented using Euler letter \(\mathcal{X}\).<br>

  <br>
  \(\mathbf{u(i)}\) represents a vector \(u\) of length i.<br><br> \(A(m,n)\) represents a matrix of shape (\(m,n\)).<br><br> \(\mathcal{T}(m,n,o)\) represents a tensor of shape \((m,n,o)\)<br>
<br>
  Matrices are a collection of vectors, so a matrix \(A(m,n)\) is represented as a collection of vectors \(a_{i}\) of length \(m\)

  $$
  
  A = \begin{bmatrix}
    \mathbf{a_{1}} & \mathbf{a_{2}}& \dots &\mathbf{a_{n} }
    \end{bmatrix}
  $$



  The notation followed is similar to to that in Kolda et al. <dt-cite key="tamara"> </dt-cite>
  </p>
  <h2>Tensor Indexing</h2>


  <p>\(\mathcal{T}[i,j,k]\) denotes the value of the tensor at position the position \((i,j,k)\)<br><br>
  Let \(\mathcal{IMG}(W,H,3)\) denote an colored image where the color channels are in order of RGB. <br>
  <br>
  \(\mathcal{IMG}[x, y, 1]\) = Intensity of Red pixel at position (x,y). <br>
  \(\mathcal{IMG}[x, y, 2]\) = Intensity of Green pixel at position (x,y). <br>
  \(\mathcal{IMG}[x, y, 3]\) = Intensity of Blue pixel at position (x,y). <br>
  </p>

  
  <h2>Vector Products</h2>

  <p>Kronecker product of two vectors is defined as follows<br>

    $$
    \begin{bmatrix}
    x_{1}  \\
    x_{2}  
    \end{bmatrix} \otimes \begin{bmatrix}
    y_{1}  \\
    y_{2}  
    \end{bmatrix} = \begin{bmatrix}
    x_{1}y_{1}  \\
    x_{1}y_{2}  \\
    x_{2}y_{1}  \\
    x_{2}y_{2} 
    \end{bmatrix} 
    $$
<h2>Matrix Products</h2>
    <p>Kronecker product of two matrices \(A(I,J)\) and \(B(K,L)\) is defined as follows<br>

    
    $$
    A \otimes B     =  \begin{bmatrix}
    a_{11}B & a_{12}B & \dots & a_{1J}B\\
    a_{21}B & a_{22}B & \dots & a_{2J}B\\
    \vdots  & \vdots  & \dots & \vdots \\
    a_{I1}B & a_{I2}B & \dots & a_{IJ}B
    \end{bmatrix} 
    $$

    which is simplified as 

    $$
    A \otimes B     =  
    \begin{bmatrix}
    a_{1} \otimes b_{1} & a_{1} \otimes b_{2} & \dots & a_{J} \otimes b_{L-1} & a_{J} \otimes b_{L}
    \end{bmatrix} 
    $$
</p>
    <h2>Tensor Flattening</h2>
    <p>The flattening of 3-dimensional tensor \(\mathcal{T}(2,2,2)\) is as follows 

    $$
    \mathcal{T_{FLAT}} = \begin{bmatrix}
    x_{1}y_{1}z_{1}  \\
    x_{1}y_{1}z_{2}  \\
    x_{1}y_{2}z_{1}  \\
    x_{1}y_{2}z_{2}  \\
    x_{2}y_{1}z_{1}  \\
    x_{2}y_{1}z_{2}  \\
    x_{2}y_{2}z_{1}  \\
    x_{2}y_{2}z_{2}
    \end{bmatrix} 
    $$
    \(x_{i}y_{j}z_{k}\) represents an element \(\mathcal{T}(i,j,k)\). The flattening can be used to represent a tensor.<br><br>
  </p>
    <h2>Relation between Kronecker Products and Tensors</h2>
    <p>Kronecker product of 3 vectors is as follows

        $$
    \begin{bmatrix}
    x_{1}  \\
    x_{2}  
    \end{bmatrix} \otimes \begin{bmatrix}
    y_{1}  \\
    y_{2}  
    \end{bmatrix} \otimes \begin{bmatrix}
    z_{1}  \\
    z_{2}  
    \end{bmatrix} = \begin{bmatrix}
    x_{1}y_{1}  \\
    x_{1}y_{2}  \\
    x_{2}y_{1}  \\
    x_{2}y_{2} 
    \end{bmatrix} \otimes \begin{bmatrix}
    z_{1}  \\
    z_{2} \end{bmatrix}  = \begin{bmatrix}
    x_{1}y_{1}z_{1}  \\
    x_{1}y_{1}z_{2}  \\
    x_{1}y_{2}z_{1}  \\
    x_{1}y_{2}z_{2}  \\
    x_{2}y_{1}z_{1}  \\
    x_{2}y_{1}z_{2}  \\
    x_{2}y_{2}z_{1}  \\
    x_{2}y_{2}z_{2}
    
    \end{bmatrix} 
    $$

    

   </p>

  <h2>Rank 1 Tensors</h2> 
  <p>Let \( \mathcal{A} (2,2,2)\) be  tensor. The tensor \(\mathcal{A}\) is said be a rank-1 tensor if \(\exists\) vectors \(\mathbf{u}(2)\), \(\mathbf{v}(2)\), \(\mathbf{w}(2)\) such that 
$$
    \mathcal{A_{FLAT}} = \begin{bmatrix}
    u_{1}  \\
    u_{2}  
    \end{bmatrix} \otimes \begin{bmatrix}
    v_{1}  \\
    v_{2}  
    \end{bmatrix} \otimes \begin{bmatrix}
    w_{1}  \\
    w_{2} 
    \end{bmatrix} 
$$

where \( u_{i},v_{i},w_{i} \) denote the \(i^{th}\) elements in the vectors \(\mathbf{u,v,w}\) respectively.
<br><br>
  It is analogous to representing a tensor \( \mathcal{A} \) using <b>six</b> values instead of <b>eight</b> values. 

  Similarly, a tensor \( \mathcal{T}(P,Q,R) \) is a rank 1 tensor if if \(\exists\) vectors \(\mathbf{u}(P)\), \(\mathbf{v}(Q)\), \(\mathbf{w}(R)\) such that

  $$
    T = \begin{bmatrix}
    u_{1}  \\
    u_{2}  \\
    \vdots \\
    u_{P}  
    \end{bmatrix} \otimes \begin{bmatrix}
    v_{1}  \\
    v_{2}  \\
    \vdots \\
    v_{Q}
    \end{bmatrix} \otimes \begin{bmatrix}
    w_{1}  \\
    w_{2}  \\
    \vdots \\
    w_{R}  
    \end{bmatrix} 
$$


  Intuitively, if a tensor  \( \mathcal{T}(P,Q,R) \) is a rank 1 tensor, it can represented using <b>P+Q+R</b> entries instead of <b>PQR</b> entries.
  </p>  

  <h2>Rank \( k \) Tensors</h2>

  <p>A tensor is said to be a rank \(k\) tensor if it can be represented as a sum of \(k\) rank-1 tensors. 

  <br>

  <br>
   Let \( \mathcal{T}(P,Q,R) \)  be a tensor. A tensor \( \mathcal{T}\) is rank-\( k \) if \( \exists\) matrices  \( U(P,k), V(Q,k), W(R,k) \) matrices such that 
<!-- 
  $$  
  U = \begin{bmatrix} u_{1}  & u_{2} & \dots & u_{k} \end{bmatrix}\\
  $$ 

  $$  
  V = \begin{bmatrix} v_{1}  & v_{2} & \dots & v_{k} \end{bmatrix} \\
  $$ 


  $$  
  W = \begin{bmatrix} w_{1}  & w_{2} & \dots & w_{k} \end{bmatrix}\\
  $$ 
 -->

    $$  
  \mathcal{T_{FLAT}} = \sum_{i=1}^{k} u_{i} \otimes v_{i} \otimes w_{i}\\
  $$ 

  Let TP(U,V,W) denote the flattened tensor generated by the kronecker product that is reshaped to the actual dimension.

  $$
  \mathcal{T} = TP(U,V,W)
  $$

  
  Finding the rank of tensor is known to be \(NP-COMPLETE\) <dt-cite key="tensorrank"></dt-cite>

  <br>
  <br><br>

<img src="images/td-image.png" style="width: 100%;height: auto">


  </p>



  <h2>Rank k Approximation</h2>
  <p>
    Let the rank of tensor \( T(P,Q,R) \) be c. 

    $$  
    \mathcal{T_{FLAT}} = \sum_{i=1}^{c} u_{i} \otimes v_{i} \otimes w_{i}\\
    $$ 


    $$  
    \mathcal{\hat{T_{FLAT}}} = \sum_{i=1}^{k} u_{i} \otimes v_{i} \otimes w_{i}\\
    $$ 

    
  \(\hat{T}\) is the rank k approximation of tensor T.
<!-- 

  <div id="half1" style="width: 50%;display: inline-block;">
    <img src="images/original.png">
  </div>


  <div id="half2" style="width: 50%">
    <img src="images/original.png">
  </div> -->

<!-- 
<div style="width: 50%; display: table;">
   <div style="display: table-row">
      <div style="width: 50%; display: table-cell;" class="text-center">
         <p style="text-align: center">Original Image </p>
         <img src="images/original.png" style="  display: block;
            margin-left: auto;
            margin-right: auto;">
         <button type="button" class="btn btn-primary" style="margin-top: 1em" onclick="increaserank()">Increase Rank</button> 
      </div>
      <div style="display: table-cell;" class="text-center">
         <p style="text-align: center"> Reconstructed Image </p>
         <img id="recon" src="images/original.png" style="  display: block;
            margin-left: auto;
            margin-right: auto;"> 
         <button type="button" class="btn btn-primary" style="margin-top: 1em" onclick="decreaserank()">Decrease Rank</button> 
      </div>
   </div>
</div> -->

</p>


<h2>CP Decomposition</h2>
<p>Given a tensor having d dimensions, the CP decomposition finds the best \(k\) rank approximation to the tensor. Given a tensor \(\mathcal{T}\) having three dimensions, then CP decomposition finds the matrices \( U(P,k), V(Q,k), W(R,k) \) such that 

      $$  
  \mathcal{\hat{T_{FLAT}}} = \sum_{i=1}^{k} u_{i} \otimes v_{i} \otimes w_{i}\\
  $$ 

  \(\mathcal{\hat{T}}\)  can be constructed from \(\mathcal{\hat{T_{FLAT}}}\) using simple reshaping. if CP decomposition algorithm is applied to a d-dimensional tensor, then it returns \(d\) matrices<br>


<br><br>Below in an interactive lllustration of representing an image \(\mathcal{T}(128,128,3)\) using \(k\) rank CP decomposition.
</p>
<div style="width: 50%" class="text-center">
  <p style="margin-bottom: 1px">Original Image</p>
  <img src="images/original.png">
  <p style="margin-bottom: 1px">No. of parameters: <b>49152</b></p>
</div>


<div style="width: 50%" class="text-center">
  <p style="margin-bottom: 1px">Reconstructed Image</p>
  <img src="images/rank_1.png" id="recon"><br>
  <p style="margin-bottom: 1px">Rank <b><span id="rankspan">1</span></b> Approximation</p>
  <p style="margin-bottom: 1px">No. of parameters used for reconstruction: <b> <span id="paramsspan">259</span></b></p>  
       <button type="button" class="btn btn-primary" style="margin-top: 1em" onclick="increaserank()">Increase Rank</button> 
       <button type="button" class="btn btn-primary" style="margin-top: 1em" onclick="decreaserank()">Decrease Rank</button> 
</div>




  </p>

  <p>
The above example shows the number of parameters used for reconstruction of the original image by varying the rank. This shows that by using a higher order tensor, one can effectively represent the original tensor, whilst using significantly fewer number of parameters.</p>


<!--   <h2>Parafac Tensor Decomposition</h2>

  <p>Given a tensor \( \T \) of rank-\(k\), express it as a sum of \(k\) rank-1 tensors. We will be discussing about Tucker decomposition and Parafac Decomposition.</p> -->

  <h2>Tensor Decomposition Visualized</h2>
  
  <p>We create a tensor \(\mathcal{T}(100,100,100)\). Each entry of the tensor \(\mathcal{T}\) is a function of its position \((i,j,k)\). The below four plots show the values at Channel 0, Channel 25, Channel 50, Channel 75 of \( \mathcal{T} \) . Lighter regions in the heatmap correspond to bigger values, and darker regions in the heatmap correspond to smaller values. Given the tensor, we wish to find the components that make up the data.</p>

<!--   <p class="text-center">Data Explorer</p>

  <div class="text-center row" >
    <p class="text-center">Channel 50</p>

<div class="col-lg-6 offset-lg-3">
  <div id="dataexplorer" >

  </div>

   <button type="button" class="btn btn-primary" style="margin-top: 1em" onclick="next_channel()">Next Channel</button> 
 <button type="button" class="btn btn-primary" style="margin-top: 1em" onclick="prev_channel()">Previous Channel</button>


</div> -->



<p>
<div  class="row" style="width: 75%">

  <div  class="col-lg-3">
    <p class="text-center">Channel 0</p>
    <div id="myDiv1">
    </div>
  </div>
  <div class="col-lg-3">
    <p class="text-center">Channel 25</p>
    <div id="myDiv2" >
    </div>
  </div>
  <div  class="col-lg-3">
    <p class="text-center">Channel 50</p>
    <div id="myDiv3">
    </div>
  </div>
  <div  class="col-lg-3">
    <p class="text-center">Channel 75</p>
    <div id="myDiv4">
    </div>
  </div>
</div>
<!-- 
<p>X-Position <input type="range" id="mean1" min="-2" max="2" style="z-index: 10000; display: inline-block;width: 50%"></p>
<p>Y-Position <input type="range" id="mean2" min="-2" max="2" style="z-index: 10000;width: "></p>
 -->

<div style="width: 50%; display: table;">
   <div style="display: table-row">
      <div style="width: 50%; display: table-cell;padding: 1em" class="text-center">
        <p>X-Position <input type="range" id="mean1" min="-2" max="2" class="slider" style="display: inline-block;"></p>
      </div>

      <div style="width: 50%; display: table-cell;padding: 1em" class="text-center">
        <p>Y-Position <input type="range" id="mean2" class="slider" min="-2" max="2" style=""></p>
      </div>
      
      <!-- <div style="width: 33%; display: table-cell;padding: 1em" class="text-center">
        
      </div>
       -->

   </div>
</div>



<p>
<div id="" class="row" style="width: 60%">
  <div class="col-lg-4 text-center">
    <p>X-Component</p>
    <div id="resultsDiv1">
    </div>
  </div>

  <div class="col-lg-4 text-center">
    <p>Y-Component</p>
    <div id="resultsDiv2">
    </div>
  </div>


  <div class="col-lg-4 text-center">
    <p>Z-Component</p>
    <div id="resultsDiv3">
    </div>
  </div>


<!-- kukunuri and bhagat wrote this -->

</div>
<p>When the tensor is decomposed using rank-1 tensor decomposition, these are the components that make up the tensor. Clearly, the X component and Y component are Gaussian. The Z-component seems to have a linear effect on the matrices. When the X-components and Y-components are shifted using the sliders, the individual components that make up the matrix also change. This example is inspired from the works of Alexej et al.  <dt-cite key="interaction"></dt-cite></p>
</p>
 <!--  <h2>Tensors and Neural Networks</h2>
  <p>
  The convolution layer in neural networks can be represented using tensors. Assume the input to be of shape \( (H, W, C) \), an image of shape \( (H, W) \) having \(C \) channels. \(N\) convolution filters having filter of shape \( (F_{x}, F_{y}) \) are used over the input. This weights matrix can be represented using a tensor T of shape \( (N, F_{x}, F_{y}, C) \). So usage of Multi-dimensional arrays for storing the weights of a neural network implies the usage of tensors.</p>


  <h2>Weight compression using Tensor Decomposition</h2>4

    <p>Explains the ideas presented in a paper</p>

  <h2>Tensor Nets</h2>

  <p>Neural networks with millions of parameters can be often hard to run on devices with low ram space.  Instead of using 4-dimensional weights tensor to represent a convolution layer, we can instead maintain four matrices whose tensor product results in the convolution filter. Need to elaborate more</p>

 -->

  <h2>Building Recommendation System using Tensor Decomposition</h2>
  <p>Recommendation systems have a ratings matrix \(R(I, J)\) of users and movies. The matrix is filled with \(NA\) for the entries which are not available. The ratings matrix is then represented using \(\hat{R} = UV^{T}\) where \(V(k,J)\) represent is the latent representation of movies and \(U(I,k)\) denote the compressed representation of users where latent dimension \(k\). The inherent assumption is that a user follows a particular behavous, which can be expressed as a linear combination of user types (student, actor, ..) and every movie is a linear combination of movie types (romance, crime, fiction, ...). The reconstruction matrix is chosen such that it minimizes the following cost function
$$
\left\Vert M * (R - UV^{T})\right\Vert^2
$$

$$
M(i,j) =  \begin{cases} 
      0 & R(i,j)=NA \\
      1 & else \\
   \end{cases}

$$

The mask ensures that on reconstruction, the original entries are closer to the actual ratings. The above matrix representation of ratings does not make use of additional information that is generally available.
<!-- about users such as geographic location, favorite genre, and language. It is also not making use
 -->
      Now, we consider a complex version of a recoomendation system with ratings represented as tensor \(\mathcal{R}(m,n,o)\) where users are spread across rows(axis=1), tv series are spread across columns(axis=2) and the episodes are spread across channels(axis=3). The missing entries are represented using \(NA\). The following questions can be answered by using this tensor formulation

      
      <ul>
        <li>What will be the rating of the next episode in the same series?</li>
        <li>Insights about the performance of episodes in a series</li>
        <li>Recommending new series based on the understanding over a large number of episodes</li>
      </ul>
    </p>
  </p>
</p>
  <p>
      <br>This representation of data is indeed an 3-Dimensional tensor, and the task is to predict the rating of a user for a given episode of a tv series. <br><br>

    The optimization procedure is similar to the matrix recommendation system. The matrices \(U(m,k),V(n,k),W(n,k)\) are chosen such that they minimize the following equation

  $$
\left\Vert M * (\mathcal{T_{FLAT}} - \sum_{i=1}^{k} u_{i} \otimes v_{i} \otimes w_{i}\\)\right\Vert^2
$$

$$
M(i,j,k) =  \begin{cases} 
      0 & R(i,j,k)=NA \\
      1 & else \\
   \end{cases}

$$

Tensors can also be extented to include complex forms of interaction between users. <dt-cite key="tensorrecommendation"></dt-cite>


</p>


<h2>Speeding up CNNs with CP tensor decomposition</h2>
<p>
In CNN, a major amount of time and memory is consumed due to the convolution operations. Typically, for example, a CNN layer with kernels of size of 3x3, 48 input channels and 128 output channels would have approximately 9x48x128 multiplications/additions per pixel. 
 <br><br>
A kernel in the CNN layers can be treated as a 4D tensor, T(n,n,p,q).
<ul>
<li>n - no. of rows/ columns of the kernel</li>
<li>p - no. of input channels for the layer</li>
<li> q - no. of output channels for the layer</li>
</ul>
</p>
<p>
Thus, tensor decomposition can be applied to obtain a low rank approximation of this tensor and the number of parameters and the additions/ multiplications can be reduced by a significant factor, thereby speeding up the convolution layer's operation.
<br><br>
We basically follow these steps as proposed by Lebedev et. al. <dt-cite key="compression"></dt-cite>for this purpose
<ul>
<li>Apply CP-Tensor decomposition (as mentioned earlier) to the 4D tensor kernel of a convolutional layer.</li>
<li>Re-apply backpropagation to the entire network to fine tune the weights</li>
<li>Repeat for next layers if required</li>
</ul>
</p>
<br><br>
<p>
The CP-decomposition of 4D tensor gives four 2D tensors and thus the new convolution is a chain of 4 smaller convolutions. 
<br><br>

Now the 4-D tensor kernel in the CNN layer is replaced by these four 2-D tensors and the full convolution is now replaced by a chain of 4 smaller convolutions.
<br><br>

<img src="images/org_conv.png"  class="center-block">
<img src="images/new_conv.png"  class="center-block">

<br>
Initially with 4-D tensor, the number of parameters would be \(n^{2}pq\) and and the convolution would require approximately the same number of “multiplications + additions” per pixel (Lebedev et. al.). After k-rank decomposition, the total no. of operations reduces to \(kn(p+q)\).
<br><br>
The above method was tested on pre-trained Character classification CNN (CharNet) test set (Lebedev et. al.). The network consists of four convolutional layers and was trained to classify 24 x 24 size images into 36 classes. The authors applied 64 rank approximations to the first two convolutional layers and were able to achieve 8.5 times speed up in terms of CPU time and an accuracy drop of just 1% compared to using the original network.

<img src="images/plot.png" class="center-block">

The plots as provided by Lebedev et al. show variations in different metrics for k-rank approximation of second and the third convolution layers (applied separately) in CharNet. <br><br>
<b>FT</b> - represents fine tuning done via backpropagation on the entire network after k-rank approximation of a convolution layer.<br><br>
<b>Accuracy Drop</b> - Percentage drop in accuracy of classification by the network after k-rank approximation. As we increase the rank, we get a better approximation of the kernel tensor and thus the accuracy drop decreases. 
<br><br>
<b>Speed Up</b> - Ratio of CPU time taken with and without tensor decomposition. Lower the rank of decomposition, lesser will be the no. of parameters involved in evaluating the convolution (given by kn(p+q), where k is the rank of decomposition), and thus lesser number of computations and thus more speed up.
<br><br>
<b>Parameter Reduction</b> - It is the ratio of number of parameters before decomposition and after decomposition for the layer. i.e
<img src="images/image1.png" class="center-block">
Clearly, it decreases with increase in k.


 </p>
<!-- 

  <h2>Neural-Network weight compression using Tensor Decomposition</h2>
  <p>
  Tensor Decomposition can also be used for compressing the weights of a neural network. Neural network weights can be compressed by quantization of weights. In an convolutional neural network, each layer of weights is an 4-Dimensional Tensor. The first index corresponds to the the applied filter. The second index corresponds to the row in the applied filter. The third index corresponds to column of the applied filter. The fourth index corresponds to the channel in the applied filter. Assuming, we are able to find the right latent factor, say r, we can compress these weights into four matrices. This reduces the memory overhead associated with storing the weights of a neural network. This is  a technique for compressing the weights of a neural network.</p>

  <h2>Tensor Nets</h2>
  <p>
  Let’s consider the inverse of the above problem. Instead of maintaining a 4-D tensor between two layers, lets assume we have four matrices corresponding to each dimension. The tensor product of these four matrices, results in a 4-D Tensor, which is the weights used by th neural network. Now, we backpropagate by computing the gradient with respect to the entries in the matrix(not the tensor) and update the entries. This way we can train neural networks in a compressed format, instead of training and then compressing. One drawback of this method is that, we need to recompute the tensor and store them in the memory or every weights update. One solution is to use a large batch size, thereby minimizing the time takes for recomputation of weights. This idea is relatively new and is coined as TensorNets. They claim that this way then compress a neural network by 200000 times, without a compromise on the accuracy.</p>



  Rithwik and Shivji wrote this

 -->

</dt-article>

<dt-appendix>
</dt-appendix>





<script>


  var rank = 1



  function reloadimage(rank){


    var elem = document.getElementById("recon")
    recon.setAttribute("src","images/rank_"+rank+".png")
    document.getElementById("rankspan").innerHTML = rank;
    document.getElementById("paramsspan").innerHTML = rank*259
    console.log("hey")


  }
  function decreaserank(){
    rank= rank-3;
    if (rank<=0){
      rank=1
    }
    reloadimage(rank)

  }


  var channel = 25;

  function next_channel() {
    channel+=1;
    if (channel>=100){
      channel=100;
    }
    change_channel()
  }

  function prev_channel(){
    channel-=1;
    if (channel<=0){
      channel=0;
    }
  }


    function increaserank(){
    rank= rank+3;
    if (rank>=97){
      rank=97
    }
    reloadimage(rank)
    change_channel()
  }


  function change_channel(){


      var data = {
       
        z: something[channel],
        zmin: 0,
        zmax: 1,
        type: 'heatmap',
        colorscale: 'YIGnBu',
        showscale:false,

    };
    
    


    
    Plotly.animate('dataexplorer', {
        data: [data]
    }, {
        transition: {
            duration: 50,
            easing: 'cubic-in-out'
        },
        frame: {
            duration: 50
        }
    });

    }


  


function gaussian(x, mean, sigma) {

    return Math.pow(2.7 / sigma, -Math.pow((x - mean) / sigma, 2))

}

function gaussian_list(list, mean, sigma) {

    var arr = []
    for (i in list) {
        arr.push(gaussian(list[i], mean, sigma))
    }
    return arr
}


function round(n, digits) {
    return Math.round(n * Math.pow(10, digits)) / Math.pow(10, digits)
}



function range(begin, end, points = 10) {

    var temp = []
    var spacing = (end - begin) / points
    
    for (i = begin; i <= end; i = i + spacing) {
        temp.push(i);
    }
    return temp
}


function twoddata(data1, data2) {

    var temp = []

    for (i in data1) {
        var temp2 = []

        for (j in data2) {
            temp2.push(data1[i] * data2[j])
        }
        temp.push(temp2)
    }
    return temp
}


function three3data(data1, data2, data3) {

    var temp = []

    var prev_res = twoddata(data1, data2)

    for (i of data3) {
        var temp1 = []
        for (j of prev_res) {
            var temp2 = []
            for (k of j) {
                temp2.push(k * i)
            }
            temp1.push(temp2)
        }
        temp.push(temp1)
    }
    return temp;
}


var data;





function update_plot() {

    var mean1 = parseInt(document.getElementById("mean1").value);
    var mean2 = parseInt(document.getElementById("mean2").value);
    console.log("mean")
    console.log(mean1);
    console.log(mean2);
    
    var arr = range(-2, 2, 101)
    var x = gaussian_list(arr, mean2, 1)
    var y = gaussian_list(arr, mean1, 1)
    var z = range(0, 1, 101)
    var something = three3data(x, y, z)



    datas = []

    for(var id=1;id<5;id++){
    var data = {
       'text':'plot'+id,
        z: something[id * 25],
        zmin: 0,
        zmax: 1,
        type: 'heatmap',
        colorscale: 'YIGnBu',
        showscale:false,
        'xaxis':'x'+id,
        'yaxis':'y'+id,
    };
    datas.push(data)
    }


    console.log("debug")
    console.log(something.length)

    for(var id=1;id<5;id++){
    Plotly.animate('myDiv'+id, {
        data: [datas[id-1]]
    }, {
        transition: {
            duration: 50,
            easing: 'cubic-in-out'
        },
        frame: {
            duration: 50
        }
    });

    }






    datas = []

    
    var trace1 = {
      x: range(0,101,101),
      y: x,
      mode: 'lines',
      'xaxis':'x1',
      'yaxis':'y1',
    };

  var trace2 = {
      x: range(0,101,101),
      y: y,
      mode: 'lines',
      'xaxis':'x2',
      'yaxis':'y2',
    };

  var trace3 = {
      x: range(0,101,101),
      y: z,
      mode: 'lines',
      'xaxis':'x3',
      'yaxis':'y3',
    };

    datas.push(trace1)
    datas.push(trace2)
    datas.push(trace3)
    


    Plotly.animate('resultsDiv1', {
        data: [trace2]
    }, {
        transition: {
            duration: 50,
            easing: 'cubic-in-out'
        },
        frame: {
            duration: 50
        }
    });


    Plotly.animate('resultsDiv2', {
        data: [trace1]
    }, {
        transition: {
            duration: 50,
            easing: 'cubic-in-out'
        },
        frame: {
            duration: 50
        }
    });


        Plotly.animate('resultsDiv3', {
        data: [trace3]
    }, {
        transition: {
            duration: 50,
            easing: 'cubic-in-out'
        },
        frame: {
            duration: 50
        }
    });





}



function delete_svgs(){

  var cusid_ele = document.getElementsByClassName('main-svg');
  cusid_ele[2].parentNode.removeChild(cusid_ele[2])
  cusid_ele[1].parentNode.removeChild(cusid_ele[1])
  //cusid_ele[0].parentNode.removeChild(cusid_ele[0])
  }

function initialize_plot() {

    datas = []

    for(var id=1;id<5;id++){
    var data = {
       'text':'plot'+id,
        z: something[(id-1) * 25],
        zmin: 0,
        zmax: 1,
        type: 'heatmap',
        colorscale: 'YIGnBu',
        showscale:false,
        'xaxis':'x'+id,
        'yaxis':'y'+id, 

    };
    datas.push(data)
    }


        var data = {
       'text':'plot'+id,
        z: something[25],
        zmin: 0,
        zmax: 1,
        type: 'heatmap',
        colorscale: 'YIGnBu',
        showscale:false,
        'xaxis':'x'+id,
        'yaxis':'y'+id, 

    };

  //         Plotly.newPlot('dataexplorer', [data],{showlegend: false, autosize: true,
  
  // height: 250,
  // width: 250,
  // margin: {
  //   l: 50,
  //   r: 50,
  //   b: 100,
  //   t: 10,
  //   pad: 2

  // }},{displayModeBar: false})
  //     // Plotly.relayout('myDiv'+id, {height: 300,})
    










    for(var id=1;id<5;id++){
      Plotly.newPlot('myDiv'+id, [datas[id-1]],{showlegend: false, autosize: true,
  
  height: 250,
  width: 250,
  margin: {
    l: 50,
    r: 50,
    b: 100,
    t: 10,
    pad: 2

  }},{displayModeBar: false})
      // Plotly.relayout('myDiv'+id, {height: 300,})
    }

    




// Plotly.newPlot('myDiv', datas, {annotations:[{text: "Z=0"},{text: "Z=0"},{text: "Z=0"},{text: "Z=0"}],
//   showlegend: false,
//   xaxis: {domain: [0, 0.19]},
//   xaxis2: {domain: [0.27, 0.46], anchor: 'y2'},
//   xaxis3: {domain: [0.54, .73], anchor: 'y3'},
//   xaxis4: {domain: [0.81, 1.0], anchor: 'y4'},
//   yaxis2: {anchor: 'x2'},
//     yaxis3: {anchor: 'x3'},
//     yaxis4: {anchor: 'x4'},
// },{displayModeBar: false})






delete_svgs()




    datas = []

  
  var trace1 = {
    x: range(0,101,101),
    y: res,
    mode: 'lines',
    'xaxis':'x1',
    'yaxis':'y1',
  };

var trace2 = {
    x: range(0,101,101),
    y: res,
    mode: 'lines',
    'xaxis':'x2',
    'yaxis':'y2',
  };

var trace3 = {
    x: range(0,101,101),
    y: z,
    mode: 'lines',
    'xaxis':'x3',
    'yaxis':'y3',
  };

  datas.push(trace1)
  datas.push(trace2)
  datas.push(trace3)
  


  Plotly.plot('resultsDiv1',[trace1],{showlegend:false,height: 250,margin: {l: 50,r: 50,b: 100,t: 10,pad: 2}},{displayModeBar: false})
  Plotly.plot('resultsDiv2',[trace2],{showlegend:false,height: 250,margin: {l: 50,r: 50,b: 100,t: 10,pad: 2}},{displayModeBar: false})
  Plotly.plot('resultsDiv3',[trace3],{showlegend:false,height: 250,margin: {l: 50,r: 50,b: 100,t: 10,pad: 2}},{displayModeBar: false})





// Plotly.plot('resultsDiv', datas, {showlegend: false,
//   xaxis: {domain: [0., 0.33]},
//   xaxis2: {domain: [0.36, 0.69], anchor: 'y2'},
//   xaxis3: {domain: [0.72,1.0], anchor: 'y3'},
//   yaxis2: {anchor: 'x2'},
//   yaxis3: {anchor: 'x3'},

  
// },{displayModeBar: false})


// Plotly.relayout('resultsDiv', {
  
//   height: 250,
// })




    // var layout = {showlegend: false,
    //   grid: {rows: 1, columns: 4, pattern: 'independent'},
    // };

//     var trace1 = {
//   x: [1, 2, 3],
//   y: [4, 5, 6],
//   type: 'scatter'
// };

// var trace2 = {
//   x: [20, 30, 40],
//   y: [50, 60, 70],
//   xaxis: 'x2',
//   yaxis: 'y2',
//   type: 'scatter'
// };

// var trace3 = {
//   x: [300, 400, 500],
//   y: [600, 700, 800],
//   xaxis: 'x3',
//   yaxis: 'y3',
//   type: 'scatter'
// };

// var trace4 = {
//   x: [4000, 5000, 6000],
//   y: [7000, 8000, 9000],
//   xaxis: 'x4',
//   yaxis: 'y4',
//   type: 'scatter'
// };

// var datas = [trace1, trace2, trace3, trace4];


    // Plotly.plot('myDiv', datas,layout,{displayModeBar: false});
}


var slider = document.getElementById("mean1");
slider.addEventListener('input', update_plot);


var slider = document.getElementById("mean2");
slider.addEventListener('input', update_plot);



var arr = range(-2, 2, 101)
var res = gaussian_list(arr, 0, 1)
var z = range(0, 1, 101)
var something = three3data(res, res, z)


initialize_plot()
update_plot()
update_plot()

// for (var k = 0; k < 20; k++) {
//     for (var i = 1; i < 5; i++) {
//         var data = [{
//             // x: [-2,-1,0,1,2],
//             // y: arr,
//             //z: twoddata(res,res),
//             z: something[i * 20 + k],
//             zmin: 0,
//             zmax: 1,
//             type: 'heatmap',
//             colorscale: 'YIGnBu',

//         }];
//         update_plot(data, i)
//     }
// }

</script>


