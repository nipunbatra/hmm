While deep neural networks have overwhelmingly established state-of-the-art results in many artificial intelligence problems, they can still be difficult to develop and debug. Recent research on deep learning understanding has focused on feature visualization 
[1, 2]
, theoretical guarantees 
[3, 4]
, model interpretability 
[5, 6]
, and generalization 
[7, 8]
.

In this work, we analyze deep neural networks from a complementary perspective, focusing on convolutional models. We are interested in understanding the extent to which input signals may affect output features, and mapping features at any part of the network to the region in the input that produces them. The key parameter to associate an output feature to an input region is the receptive field of the convolutional network, which is defined as the size of the region in the input that produces the feature.

As our first contribution, we present a mathematical derivation and an efficient algorithm to compute receptive fields of modern convolutional neural networks. Previous work 
[9, 10]
 discussed receptive field computation for simple convolutional networks where there is a single path from the input to the output, providing recurrence equations that apply to this case. In this work, we revisit these derivations to obtain a closed-form expression for receptive field computation in the single-path case. Furthermore, we extend receptive field computation to modern convolutional networks where there may be multiple paths from the input to the output. To the best of our knowledge, this is the first exposition of receptive field computation for such recent convolutional architectures.

Today, receptive field computations are needed in a variety of applications. For example, for the computer vision task of object detection, it is important to represent objects at multiple scales in order to recognize small and large instances; understanding a convolutional feature’s span is often required for that goal (e.g., if the receptive field of the network is small, it may not be able to recognize large objects). However, these computations are often done by hand, which is both tedious and error-prone. This is because there are no libraries to compute these parameters automatically. As our second contribution, we fill the void by introducing an open-source library which handily performs the computations described here. The library is integrated into the Tensorflow codebase and can be easily employed to analyze a variety of models, as presented in this article.

We expect these derivations and open-source code to improve the understanding of complex deep learning models, leading to more productive machine learning research.

Overview of the article
We consider fully-convolutional neural networks, and derive their receptive field size and receptive field locations for output features with respect to the input signal. While the derivations presented here are general enough for any type of signal used at the input of convolutional neural networks, we use images as a running example, referring to modern computer vision architectures when appropriate.

First, we derive closed-form expressions when the network has a single path from input to output (as in AlexNet 
[11]
 or VGG 
[12]
). Then, we discuss the more general case of arbitrary computation graphs with multiple paths from the input to the output (as in ResNet 
[13]
 or Inception 
[14]
). We consider potential alignment issues that arise in this context, and explain an algorithm to compute the receptive field size and locations.

Finally, we analyze the receptive fields of modern convolutional neural networks, showcasing results obtained using our open-source library.

Problem setup
Consider a fully-convolutional network (FCN) with 
L
 layers, 
l
=
1
,
2
,
…
,
L
. Define feature map 
f
l
∈
R
h
l
×
w
l
×
d
l
 to denote the output of the 
l
-th layer, with height 
h
l
, width 
w
l
 and depth 
d
l
. We denote the input image by 
f
0
. The final output feature map corresponds to 
f
L
.

To simplify the presentation, the derivations presented in this document consider 
1
-dimensional input signals and feature maps. For higher-dimensional signals (e.g., 
2
D images), the derivations can be applied to each dimension independently. Similarly, the figures depict 
1
-dimensional depth, since this does not affect the receptive field computation.

Each layer 
l
’s spatial configuration is parameterized by 4 variables, as illustrated in the following figure:

k
l
: kernel size (positive integer)
s
l
: stride (positive integer)
p
l
: padding applied to the left side of the input feature map (non-negative integer) 1
q
l
: padding applied to the right side of the input feature map (non-negative integer)
Kernel Size (kl): 2
Left Padding (pl): 1
Right Padding (ql): 1
Stride (sl): 3
flfl-1klklklslplql
We consider layers whose output features depend locally on input features: e.g., convolution, pooling, or elementwise operations such as non-linearities, addition and filter concatenation. These are commonly used in state-of-the-art networks. We define elementwise operations to have a “kernel size” of 
1
, since each output feature depends on a single location of the input feature maps.

Our notation is further illustrated with the simple network below. In this case, 
L
=
4
 and the model consists of a convolution, followed by ReLU, a second convolution and max-pooling. 2

f0f1f2f3f4Convolution
Kernel Size (k1): 3 

Padding (p1, q1): 1 

Stride (s1): 2 

ReLU
Kernel Size (k2): 1
Padding (p2, q2): 0
Stride (s2): 1
Convolution
Kernel Size (k3): 1 

Padding (p3, q3): 0 

Stride (s3): 1 

Max Pooling
Kernel Size (k4): 3 

Padding (p4, q4): 1 

Stride (s4): 2 

Single-path networks
In this section, we compute recurrence and closed-form expressions for fully-convolutional networks with a single path from input to output (e.g., AlexNet 
[11]
 or VGG 
[12]
).

Computing receptive field size
Define 
r
l
 as the receptive field size of the final output feature map 
f
L
, with respect to feature map 
f
l
. In other words, 
r
l
 corresponds to the number of features in feature map 
f
l
 which contribute to generate one feature in 
f
L
. Note that 
r
L
=
1
.

As a simple example, consider layer 
L
, which takes features 
f
L
−
1
 as input, and generates 
f
L
 as output. Here is an illustration:

Kernel Size (kL): 2
Padding (pL, qL): 0
Stride (sL): 3
kLkLfL-1fL
It is easy to see that 
k
L
 features from 
f
L
−
1
 can influence one feature from 
f
L
, since each feature from 
f
L
 is directly connected to 
k
L
 features from 
f
L
−
1
. So, 
r
L
−
1
=
k
L
.

Now, consider the more general case where we know 
r
l
 and want to compute 
r
l
−
1
. Each feature 
f
l
 is connected to 
k
l
 features from 
f
l
−
1
.

First, consider the situation where 
k
l
=
1
: in this case, the 
r
l
 features in 
f
l
 will cover 
r
l
−
1
=
s
l
⋅
r
l
−
(
s
l
−
1
)
 features in in 
f
l
−
1
. This is illustrated in the figure below, where 
r
l
=
2
 (highlighted in red). The first term 
s
l
⋅
r
l
 (green) covers the entire region where the features come from, but it will cover 
s
l
−
1
 too many features (purple), which is why it needs to be deducted. 3

Kernel Size (kl): 1
Padding (pl, ql): 0
Stride (sl): 3
sl⋅rlsl-1fl-1flrl=2
For the case where 
k
l
>
1
, we just need to add 
k
l
−
1
 features, which will cover those from the left and the right of the region. For example, if we use a kernel size of 
5
 (
k
l
=
5
), there would be 
2
 extra features used on each side, adding 
4
 in total. If 
k
l
 is even, this works as well, since the left and right padding will add to 
k
l
−
1
. 4

Kernel Size (kl): 5
Padding (pl, ql): 0
Stride (sl): 3
sl⋅rl - (sl-1)(kl-1)/2(kl-1)/2fl-1flrl=2
So, we obtain the general recurrence equation (which is first-order, non-homogeneous, with variable coefficients ):

r
l
−
1
=
s
l
⋅
r
l
+
(
k
l
−
s
l
)
 
  
(1)
 
This equation can be used in a recursive algorithm to compute the receptive field size of the network, 
r
0
. However, we can do even better: we can solve the recurrence equation and obtain a solution in terms of the 
k
l
’s and 
s
l
’s:

r
0
=
L
∑
l
=
1
 
(
(
k
l
−
1
)
l
−
1
∏
i
=
1
 
s
i
)
+
1
  
(2)
 
This expression makes intuitive sense, which can be seen by considering some special cases. For example, if all kernels are of size 1, naturally the receptive field is also of size 1. If all strides are 1, then the receptive field will simply be the sum of 
(
k
l
−
1
)
 over all layers, plus 1, which is simple to see. If the stride is greater than 1 for a particular layer, the region increases proportionally for all layers below that one. Finally, note that padding does not need to be taken into account for this derivation.

Computing receptive field region in input image
While it is important to know the size of the region which generates one feature in the output feature map, in many cases it is also critical to precisely localize the region which generated a feature. For example, given feature 
f
L
(
i
,
j
)
, what is the region in the input image which generated it? This is addressed in this section.

Let’s denote 
u
l
 and 
v
l
 the left-most and right-most coordinates (in 
f
l
) of the region which is used to compute the desired feature in 
f
L
. In these derivations, the coordinates are zero-indexed (i.e., the first feature in each map is at coordinate 
0
). Note that 
u
L
=
v
L
 corresponds to the location of the desired feature in 
f
L
. The figure below illustrates a simple 2-layer network, where we highlight the region in 
f
0
 which is used to compute the first feature from 
f
2
. Note that in this case the region includes some padding. In this example, 
u
2
=
v
2
=
0
, 
u
1
=
0
,
v
1
=
1
, and 
u
0
=
−
1
,
v
0
=
4
.

Kernel Size (k1): 3
Padding (p1, q1): 1
Stride (s1): 3
Kernel Size (k2): 2
Padding (p2, q2): 0
Stride (s2): 1
u0 = -1v0 = 4f0f1f2
We’ll start by asking the following question: given 
u
l
,
v
l
, can we compute 
u
l
−
1
,
v
l
−
1
?

Start with a simple case: let’s say 
u
l
=
0
 (this corresponds to the first position in 
f
l
). In this case, the left-most feature 
u
l
−
1
 will clearly be located at 
−
p
l
, since the first feature will be generated by placing the left end of the kernel over that position. If 
u
l
=
1
, we’re interested in the second feature, whose left-most position 
u
l
−
1
 is 
−
p
l
+
s
l
; for 
u
l
=
2
, 
u
l
−
1
=
−
p
l
+
2
⋅
s
l
; and so on. In general:

u
l
−
1
=
−
p
l
+
u
l
⋅
s
l
v
l
−
1
=
−
p
l
+
v
l
⋅
s
l
+
k
l
−
1
  
(3)
(4)
 
where the computation of 
v
l
 differs only by adding 
k
l
−
1
, which is needed since in this case we want to find the right-most position.

Note that these expressions are very similar to the recursion derived for the receptive field size 
(1)
. Again, we could implement a recursion over the network to obtain 
u
l
,
v
l
 for each layer; but we can also solve for 
u
0
,
v
0
 and obtain closed-form expressions in terms of the network parameters:

u
0
=
u
L
L
∏
i
=
1
 
s
i
−
L
∑
l
=
1
 
p
l
l
−
1
∏
i
=
1
 
s
i
  
(5)
 
This gives us the left-most feature position in the input image as a function of the padding (
p
l
) and stride (
s
l
) applied in each layer of the network, and of the feature location in the output feature map (
u
L
).

And for the right-most feature location 
v
0
:

v
0
=
v
L
L
∏
i
=
1
 
s
i
−
L
∑
l
=
1
 
(
1
+
p
l
−
k
l
)
l
−
1
∏
i
=
1
 
s
i
  
(6)
 
Note that, different from 
(5)
, this expression also depends on the kernel sizes (
k
l
) of each layer.

Relation between receptive field size and region. You may be wondering that the receptive field size 
r
0
 must be directly related to 
u
0
 and 
v
0
. Indeed, this is the case; it is easy to show that 
r
0
=
v
0
−
u
0
+
1
, which we leave as a follow-up exercise for the curious reader. To emphasize, this means that we can rewrite 
(6)
 as:

v
0
=
u
0
+
r
0
−
1
  
(7)
 
Effective stride and effective padding. To compute 
u
0
 and 
v
0
 in practice, it is convenient to define two other variables, which depend only on the paddings and strides of the different layers:

effective stride 
S
l
=
∏
L
i
=
l
+
1
s
i
: the stride between a given feature map 
f
l
 and the output feature map 
f
L
effective padding 
P
l
=
∑
L
m
=
l
+
1
p
m
∏
m
−
1
i
=
l
+
1
s
i
: the padding between a given feature map 
f
l
 and the output feature map 
f
L
With these definitions, we can rewrite 
(5)
 as:

u
0
=
−
P
0
+
u
L
⋅
S
0
  
(8)
 
Note the resemblance between 
(8)
 and 
(3)
. By using 
S
l
 and 
P
l
, one can compute the locations 
u
l
,
v
l
 for feature map 
f
l
 given the location at the output feature map 
u
L
. When one is interested in computing feature locations for a given network, it is handy to pre-compute three variables: 
P
0
,
S
0
,
r
0
. Using these three, one can obtain 
u
0
 using 
(8)
 and 
v
0
 using 
(7)
. This allows us to obtain the mapping from any output feature location to the input region which influences it.

It is also possible to derive recurrence equations for the effective stride and effective padding. It is straightforward to show that:

S
l
−
1
=
s
l
⋅
S
l
P
l
−
1
=
s
l
⋅
P
l
+
p
l
  
(9)
(10)
 
These expressions will be handy when deriving an algorithm to solve the case for arbitrary computation graphs, presented in the next section.

Center of receptive field region. It is also interesting to derive an expression for the center of the receptive field region which influences a particular output feature. This can be used as the location of the feature in the input image (as done for recent deep learning-based local features 
[15]
, for example).

We define the center of the receptive field region for each layer 
l
 as 
c
l
=
u
l
+
v
l
2
. Given the above expressions for 
u
0
,
v
0
,
r
0
, it is straightforward to derive 
c
0
 (remember that 
u
L
=
v
L
):

c
0
=
u
L
L
∏
i
=
1
 
s
i
−
L
∑
l
=
1
 
(
p
l
−
k
l
−
1
2
)
l
−
1
∏
i
=
1
 
s
i
=
u
L
⋅
S
0
−
L
∑
l
=
1
 
(
p
l
−
k
l
−
1
2
)
l
−
1
∏
i
=
1
 
s
i
=
−
P
0
+
u
L
⋅
S
0
+
(
r
0
−
1
2
)
  
(11)
 
This expression can be compared to 
(8)
 to observe that the center is shifted from the left-most pixel by 
r
0
−
1
2
, which makes sense. Note that the receptive field centers for the different output features are spaced by the effective stride 
S
0
, as expected. Also, it is interesting to note that if 
p
l
=
k
l
−
1
2
 for all 
l
, the centers of the receptive field regions for the output features will be aligned to the first image pixel and located at 
0
,
S
0
,
2
S
0
,
3
S
0
,
…
 (note that in this case all 
k
l
’s must be odd).

Other network operations. The derivations provided in this section cover most basic operations at the core of convolutional neural networks. A curious reader may be wondering about other commonly-used operations, such as dilation, upsampling, etc. You can find a discussion on these in the appendix.

Arbitrary computation graphs
Most state-of-the-art convolutional neural networks today (e.g., ResNet 
[13]
 or Inception 
[14]
) rely on models where each layer may have more than one input, which means that there might be several different paths from the input image to the final output feature map. These architectures are usually represented using directed acyclic computation graphs, where the set of nodes 
L
 represents the layers and the set of edges 
E
 encodes the connections between them (the feature maps flow through the edges).

The computation presented in the previous section can be used for each of the possible paths from input to output independently. The situation becomes trickier when one wants to take into account all different paths to find the receptive field size of the network and the receptive field regions which correspond to each of the output features.

Alignment issues. The first potential issue is that one output feature may be computed using misaligned regions of the input image, depending on the path from input to output. Also, the relative position between the image regions used for the computation of each output feature may vary. As a consequence, the receptive field size may not be shift-invariant  . This is illustrated in the figure below with a toy example, in which case the centers of the regions used in the input image are different for the two paths from input to output.


Kernel Size (k1): 5
Left Pad (p1): 2
Right Pad (q1): 1
Stride (s1): 2
Kernel Size (k2): 3
Left Pad (p2): 0
Right Pad (q2): 0
Stride (s2): 1
Kernel Size (k3): 3
Left Pad (p3): 0
Right Pad (q3): 0
Stride (s3): 1
Add
In this example, padding is used only for the left branch. The first three layers are convolutional, while the last layer performs a simple addition. The relative position between the receptive field regions of the left and right paths is inconsistent for different output features, which leads to a lack of alignment (this can be seen by hovering over the different output features). Also, note that the receptive field size for each output feature may be different. For the second feature from the left, 
6
 input samples are used, while only 
5
 are used for the third feature. This means that the receptive field size may not be shift-invariant when the network is not aligned.

For many computer vision tasks, it is highly desirable that output features be aligned: “image-to-image translation” tasks (e.g., semantic segmentation, edge detection, surface normal estimation, colorization, etc), local feature matching and retrieval, among others.

When the network is aligned, all different paths lead to output features being centered consistently in the same locations. All different paths must have the same effective stride. It is easy to see that the receptive field size will be the largest receptive field among all possible paths. Also, the effective padding of the network corresponds to the effective padding for the path with largest receptive field size, such that one can apply 
(8)
, 
(11)
 to localize the region which generated an output feature.

The figure below gives one simple example of an aligned network. In this case, the two different paths lead to the features being centered at the same locations. The receptive field size is 
3
, the effective stride is 
4
 and the effective padding is 
1
.


Kernel Size (k1): 1
Left Pad (p1): 0
Right Pad (q1): 0
Stride (s1): 4
Kernel Size (k2): 3
Left Pad (p2): 1
Right Pad (q2): 0
Stride (s2): 2
Kernel Size (k3): 1
Left Pad (p3): 0
Right Pad (q3): 0
Stride (s3): 2
Add
Alignment criteria  . More precisely, for a network to be aligned at every layer, we need every possible pair of paths 
i
 and 
j
 to have 
c
(
i
)
l
=
c
(
j
)
l
 for any layer 
l
 and output feature 
u
L
. For this to happen, we can see from 
(11)
 that two conditions must be satisfied:

S
(
i
)
l
=
S
(
j
)
l
−
P
(
i
)
l
+
⎛
⎝
r
(
i
)
l
−
1
2
⎞
⎠
=
−
P
(
j
)
l
+
⎛
⎝
r
(
j
)
l
−
1
2
⎞
⎠
  
(12)
(13)
 
for all 
i
,
j
,
l
.

Algorithm for computing receptive field parameters: sketch. It is straightforward to develop an efficient algorithm that computes the receptive field size and associated parameters for such computation graphs. Naturally, a brute-force approach is to use the expressions presented above to compute the receptive field parameters for each route from the input to output independently, coupled with some bookkeeping in order to compute the parameters for the entire network. This method has a worst-case complexity of 
O
(
|
E
|
×
|
L
|
)
.

But we can do better. Start by topologically sorting the computation graph. The sorted representation arranges the layers in order of dependence: each layer’s output only depends on layers that appear before it. By visiting layers in reverse topological order, we ensure that all paths from a given layer 
l
 to the output layer 
L
 have been taken into account when 
l
 is visited. Once the input layer 
l
=
0
 is reached, all paths have been considered and the receptive field parameters of the entire model are obtained. The complexity of this algorithm is 
O
(
|
E
|
+
|
L
|
)
, which is much better than the brute-force alternative.

As each layer is visited, some bookkeeping must be done in order to keep track of the network’s receptive field parameters. In particular, note that there might be several different paths from layer 
l
 to the output layer 
L
. In order to handle this situation, we keep track of the parameters for 
l
 and update them if a new path with larger receptive field is found, using expressions 
(1)
, 
(9)
 and 
(10)
. Similarly, as the graph is traversed, it is important to check that the network is aligned. This can be done by making sure that the receptive field parameters of different paths satisfy 
(12)
 and 
(13)
.

Discussion: receptive fields of modern networks
In this section, we present the receptive field parameters of modern convolutional networks 5  , which were computed using the new open-source library (script here). The pre-computed parameters for AlexNet 
[11]
, VGG 
[12]
, ResNet 
[13]
, Inception 
[14]
 and MobileNet 
[16]
 are presented in the table below. For a more comprehensive list, including intermediate network end-points, see this table.

ConvNet Model	Receptive
Field (r)	Effective
Stride (S)	Effective
Padding (P)	Model Year
alexnet_v2	195	32	64	2014
vgg_16	212	32	90	2014
mobilenet_v1	315	32	126	2017
mobilenet_v1_075	315	32	126	2017
resnet_v1_50	483	32	239	2015
inception_v2	699	32	318	2015
resnet_v1_101	1027	32	511	2015
inception_v3	1311	32	618	2015
resnet_v1_152	1507	32	751	2015
resnet_v1_200	1763	32	879	2015
inception_v4	2071	32	998	2016
inception_resnet_v2	3039	32	1482	2016
As models evolved, from AlexNet, to VGG, to ResNet and Inception, the receptive fields increased (which is a natural consequence of the increased number of layers). In the most recent networks, the receptive field usually covers the entire input image: this means that the context used by each feature in the final output feature map includes all of the input pixels.

We can also relate the growth in receptive fields to increased classification accuracy. The figure below plots ImageNet top-1 accuracy as a function of the network’s receptive field size, for the same networks listed above. The circle size for each data point is proportional to the number of floating-point operations (FLOPs) for each architecture.

alexnet
vgg_16
inception
inception_resnet
1/2
0
500
1,000
1,500
2,000
2,500
3,000
3,500
55%
60%
65%
70%
75%
80%
85%
Receptive field size (pixels)
ImageNet top-1 accuracy
ID	Receptive field size (pixels)	ImageNet top-1 accuracy	Family	FLOPS (Billion)
alexnet_v2	195	0.572	alexnet	1.38
vgg_16	212	0.715	vgg_16	30.71
inception_v2	699	0.739	inception	3.88
inception_v3	1,311	0.78	inception	5.69
inception_v4	2,071	0.802	inception	12.27
inception_resnet_v2	3,039	0.804	inception_resnet	12.96
resnet_v1_50	483	0.752	resnet	6.97
resnet_v1_101	1,027	0.764	resnet	14.4
resnet_v1_152	1,507	0.768	resnet	21.82
mobilenet_v1	315	0.709	mobilenet	1.14
We observe a logarithmic relationship between classification accuracy and receptive field size, which suggests that large receptive fields are necessary for high-level recognition tasks, but with diminishing rewards. For example, note how MobileNets achieve high recognition performance even if using a very compact architecture: with depth-wise convolutions, the receptive field is increased with a small compute footprint. In comparison, VGG-16 requires 27X more FLOPs than MobileNets, but produces a smaller receptive field size; even if much more complex, VGG’s accuracy is only slightly better than MobileNet’s. This suggests that networks which can efficiently generate large receptive fields may enjoy enhanced recognition performance.

Let us emphasize, though, that the receptive field size is not the only factor contributing to the improved performance mentioned above. Other factors play a very important role: network depth (i.e., number of layers) and width (i.e., number of filters per layer), residual connections, batch normalization, to name only a few. In other words, while we conjecture that a large receptive field is necessary, by no means it is sufficient. 6

Finally, note that a given feature is not equally impacted by all input pixels within its receptive field region: the input pixels near the center of the receptive field have more “paths” to influence the feature, and consequently carry more weight. The relative importance of each input pixel defines the effective receptive field of the feature. Recent work 
[17]
 provides a mathematical formulation and a procedure to measure effective receptive fields, experimentally observing a Gaussian shape, with the peak at the receptive field center. Better understanding the relative importance of input pixels in convolutional neural networks is an active research topic.